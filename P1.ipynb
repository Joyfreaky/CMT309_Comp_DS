{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Data Science Portfolio - Part I (30 marks)\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7Pm74v1u4d6G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/joydipb/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/joydipb/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/joydipb/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\") # to ignore warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WfNsDQ253nzJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\n"
          ]
        }
      ],
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "\n",
        "\n",
        "df = pd.read_csv('data_portfolio_21.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CNfbxg2X3nzK"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       author            posted_at  num_comments  score selftext  \\\n",
              "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
              "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
              "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
              "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
              "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
              "\n",
              "  subr_created_at              subr_description  \\\n",
              "0      2009-04-29  Subreddit about Donald Trump   \n",
              "1      2009-04-29  Subreddit about Donald Trump   \n",
              "2      2009-04-29  Subreddit about Donald Trump   \n",
              "3      2009-04-29  Subreddit about Donald Trump   \n",
              "4      2009-04-29  Subreddit about Donald Trump   \n",
              "\n",
              "                                       subr_faved_by  subr_numb_members  \\\n",
              "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "\n",
              "   subr_numb_posts    subreddit  \\\n",
              "0           796986  donaldtrump   \n",
              "1           796986  donaldtrump   \n",
              "2           796986  donaldtrump   \n",
              "3           796986  donaldtrump   \n",
              "4           796986  donaldtrump   \n",
              "\n",
              "                                               title  total_awards_received  \\\n",
              "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
              "1                                Joe Biden's America                      0   \n",
              "2  4 more years and we can erase his legacy for g...                      0   \n",
              "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
              "4                                     LOOK HERE, FAT                      0   \n",
              "\n",
              "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
              "0          1.00            4661         2012-11-09          -0.658599  \n",
              "1          0.67            4661         2012-11-09          -0.658599  \n",
              "2          1.00            4661         2012-11-09          -0.658599  \n",
              "3          1.00            4661         2012-11-09          -0.658599  \n",
              "4          0.88            4661         2012-11-09          -0.658599  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head() # check the first few rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing (10 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUtCUL853Ln"
      },
      "source": [
        "### P1.1.1 - Faved by as lists (3 marks)\n",
        "\n",
        "The column `subr_faved_by` contains an array of values (names of redditors who added the subreddit to which the current post was submitted), but unfortunately they are in text format, and you would not be able to process them properly without converting them to a suitable python type. You must convert these string values to Python lists, going from\n",
        "\n",
        "```python\n",
        "'[\"user1\", \"user2\" ... ]'\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```python\n",
        "[\"user1\", \"user2\" ... ]\n",
        "```\n",
        "\n",
        "**What to implement:** Implement a function `transform_faves(df)` which takes as input the original dataframe and returns the same dataframe, but with one additional column called `subr_faved_by_as_list`, where you have the same information as in `subr_faved_by`, but as a python list instead of a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RJLEddGE56qw"
      },
      "outputs": [],
      "source": [
        "def transform_faves(df):\n",
        "    df['subr_faved_by_as_list'] = [literal_eval(x) for x in df['subr_faved_by']] # this is a list of lists\n",
        "   \n",
        "    return df   # return the dataframe\n",
        "\n",
        "df = transform_faves(df) # run the function on the dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Merge titles and text bodies (4 marks)\n",
        "\n",
        "All Reddit posts need to have a title, but a text body is optional. However, we want to be able to access all free text information for each post without having to look at two columns every time.\n",
        "\n",
        "**What to implement**: A function `concat(df)` that will take as input the original dataframe and will return it with an additional column called `full_text`, which will concatenate `title` and `selftext` columns, but with the following restrictions:\n",
        "\n",
        "- 1) Wrap the title between `<title>` and `</title>` tags.\n",
        "- 2) Add a new line (`\\n`) between title and selftext, but only in cases where you have both values (see instruction 4).\n",
        "- 3) Wrap the selftext between `<selftext>` and `</selftext>`.\n",
        "- 4) You **must not** include the tags in points (1) or (3) if the values for these columns is missing. We will consider a missing value either an empty value (empty string) or a string of only one character (e.g., an emoji). Also, the value of a `full_text` column must not end in the new line character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RsmY-JB39N2m"
      },
      "outputs": [],
      "source": [
        "def concat(df):\n",
        "    df[\"full_text\"] = pd.Series().astype(str) # create a new column\n",
        "    for i in range(len(df)):\n",
        "        if(len(df['selftext'][i]) == 0): # if there is no selftext\n",
        "            df['full_text'][i] = \"<title>\" + df['title'][i] + \"</title>\"   # add the title\n",
        "        else: # if there is selftext\n",
        "            df['full_text'][i] = \"<title>\" + df['title'][i] + \"</title>\\n<selftext>\" + df['selftext'][i] + \"</selftext>\" # add the title and selftext\n",
        "                \n",
        "    return df # return the dataframe\n",
        "    \n",
        "df = concat(df) # run the function on the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADWvbAIe4TVd"
      },
      "source": [
        "### P1.1.3 - Enrich posts (3 marks)\n",
        "\n",
        "We would like to augment our text data with linguistic information. To this end, we will _tokenize_, apply _part-of-speech tagging_, and then we will _lower case_ all the posts.\n",
        "\n",
        "**What to implement**: A function `enrich_posts(df)` that will take as input the original dataframe and will return it with **two** additional columns: `enriched_title` and `enriched_selftext`. These columns will contain tokenized, pos-tagged and lower cased versions of the original text. **You must implement them in this order**, because the pos tagger uses casing information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_nDnaSwI46T_"
      },
      "outputs": [],
      "source": [
        "def enrich_posts(df):\n",
        "    df[\"enriched_title\"] = pd.Series().astype(str) # create a new column\n",
        "    df[\"enriched_selftext\"] = pd.Series().astype(str) # create a new column\n",
        "    for i in range(len(df)):\n",
        "        df['enriched_title'][i] = word_tokenize(text=df['title'][i]) # tokenize the title\n",
        "        df['enriched_title'][i] = nltk.pos_tag(df['enriched_title'][i]) # tag the title\n",
        "        df['enriched_title'][i] = [(ls.lower(), cat) for ls, cat in df['enriched_title'][i]] # lowercase the title and tag it\n",
        "\n",
        "        df['enriched_selftext'][i] = word_tokenize(text=df['selftext'][i]) # tokenize the selftext\n",
        "        df['enriched_selftext'][i] = nltk.pos_tag(df['enriched_selftext'][i]) # tag the selftext\n",
        "        df['enriched_selftext'][i] = [(ls.lower(), cat) for ls, cat in df['enriched_selftext'][i]] # lowercase the selftext and tag it \n",
        "    return df # return the dataframe\n",
        "\n",
        "df = enrich_posts(df) # run the function on the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (12 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Users with best scores (3 marks)\n",
        "\n",
        "- Find the users with the highest aggregate scores (over all their posts) for the whole dataset. You should restrict your results to only those whose aggregated score is above 10,000 points, in descending order. Your code should generate a dictionary of the form `{author:aggregated_scores ... }`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RhW8Rr6QSXDj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('DaFunkJunkie', 250375), ('None', 218846), ('SUPERGUESSOUS', 211611), ('jigsawmap', 210784), ('chrisdh79', 143538), ('hildebrand_rarity', 122464), ('iSlingShlong', 118595), ('hilltopye', 81245), ('tefunka', 79560), ('OldFashionedJizz', 64398), ('JLBesq1981', 58235), ('rspix000', 57107), ('Wagamaga', 47989), ('stem12345679', 47455), ('TheJeck', 26058), ('TheGamerDanYT', 25357), ('TrumpSharted', 21154), ('NotsoPG', 18518), ('SonictheManhog', 18116), ('BlanketMage', 13677), ('NewAltWhoThis', 12771), ('kevinmrr', 11900), ('Dajakesta0624', 11613), ('apocalypticalley', 10382)]\n"
          ]
        }
      ],
      "source": [
        "df_preserve = df.copy() # make a copy of the dataframe\n",
        "import gc\n",
        "for i in range(len(df)-1): # for each row\n",
        "    if(df['author'][i+1] == df['author'][i]): # if the next row is the same author\n",
        "        df['score'][i+1] += df['score'][i] # add the score\n",
        "        df = df.drop(index=i) # drop the current row\n",
        "        \n",
        "    else: # if the next row is not the same author\n",
        "        pass # do nothing\n",
        "\n",
        "df = df.reset_index() # reset the index\n",
        "\n",
        "best_score_dict = defaultdict(int) # create a dictionary\n",
        "\n",
        "for i in range(len(df)): # for each row\n",
        "    if(df['score'][i] >= 10000): # if the score is greater than 10000\n",
        "        best_score_dict[df['author'][i]] += df['score'][i] # add the score to the dictionary\n",
        "                \n",
        "    else: # if the score is less than 10000\n",
        "        pass # do nothing\n",
        "\n",
        "df = df_preserve.copy() # make a copy of the dataframe\n",
        "\n",
        "# Sort the dictionary by value\n",
        "best_score_dict = sorted(best_score_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "del df_preserve # delete the copy\n",
        "gc.collect() # garbage collect\n",
        "\n",
        "print(best_score_dict) # print the dictionary\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOFrPFQT5cZ"
      },
      "source": [
        "### P1.2.2 - Awarded posts (3 marks)\n",
        "\n",
        "Find the number of posts that have received at least one award. Your query should return only one value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0fVuaWmmUGVW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of post that recieved atleast one award are:  119\n"
          ]
        }
      ],
      "source": [
        "award_count = 0 # create a counter\n",
        "for i in range(len(df)): # for each row\n",
        "    if(df['total_awards_received'][i] >= 1): # if the award count is greater than 1\n",
        "        award_count+=1 # add 1 to the counter\n",
        "    else: # if the award count is less than 1\n",
        "        pass # do nothing\n",
        "print(\"Number of post that recieved atleast one award are: \", award_count) # print the counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 Find Covid (3 marks)\n",
        "\n",
        "Find the name and description of all subreddits where the name starts with `Covid` or `Corona` and the description contains `covid` or `Covid` anywhere. Your code should generate a dictionary of the form#\n",
        "\n",
        "```python\n",
        "  {'Coronavirus':'Place to discuss all things COVID-related',\n",
        "  ...\n",
        "  }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w6fIWO8BUhu3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'str'>, {'donaldtrump': 'Subreddit about Donald Trump', 'Coronavirus': 'Place to discuss all things COVID-related', 'China_Flu': 'COVID-19 (2019-nCoV) Wuhan Coronavirus Information', 'conspiracy': \"**The conspiracy subreddit is a thinking ground. Above all else, we respect everyone's opinions and ALL religious beliefs and creeds. We hope to challenge issues which have captured the public’s imagination, from JFK and UFOs to 9/11. This is a forum for free thinking, not hate speech. Respect other views and opinions, and keep an open mind.** **Our intentions are aimed towards a fairer, more transparent world and a better future for everyone.**\", 'CoronavirusCA': 'Tracking the Coronavirus/Covid-19 outbreak in California', 'PublicFreakout': 'A subreddit dedicated to people freaking out, melting down, losing their cool, or being weird in public.', 'JoeBiden': \"President Joe Biden | We are the United States of America. There is not a single thing we cannot do. We need to tackle our nation's challenges and build our country back better. Join us!!!\", 'CoronavirusUS': 'USA/Canada specific information on the coronavirus (SARS-CoV-2) that causes coronavirus disease 2019 (COVID-19)', 'LockdownSkepticism': 'Examining the empirical basis for mandatory lockdown policies in both the physical and social sciences. We are concerned about the impact of COVID-19 lockdowns/quarantines on our freedoms, human rights, physical and mental health, and economy. We are skeptical of ongoing lockdowns as an effective way to manage the coronavirus pandemic. This is a non-partisan, non-racist, multidisciplinary, global sub. Propagating conspiracy theories is strictly against sub rules.', 'COVID19': 'In December 2019, SARS-CoV-2, the virus causing the disease COVID-19, emerged in the city of Wuhan, China. This subreddit seeks to facilitate scientific discussion of this global public health threat.', 'CoronavirusUK': 'Spreading news, advice and media following the UK’s spread of the virus.', 'NoNewNormal': 'Discussion about skepticism regarding the \"new normal\" that stems from the coronavirus (COVID-19) pandemic.', 'CoronavirusCirclejerk': 'Sub for corona circlejerking, memes, and sharing and discussing stupid, ridiculous, and amusing posts and discussion from the various panic-filled and alarmist Coronavirus-related subs.', 'COVID': 'COVID-19 News, Etc.', 'CoronavirusDownunder': 'This subreddit is a place to share news, information, resources, and support that relate to the novel coronavirus SARS-CoV-2 and the disease it causes called COVID-19. The primary focus of this sub is to actively monitor the situation in Australia, but all posts on international news and other virus-related topics are welcome, to the extent they are beneficial in keeping those in Australia informed.', 'CovIdiots': 'Idiocy during the pandemic.', 'politics': '/r/Politics is for news and discussion about U.S. politics.', 'CanadaCoronavirus': 'Information and discussion related to the spread and impact of Coronavirus in Canada.', 'NoLockdownsNoMasks': 'Articles / stories related to lockdowns & masks not working / being harmful (prolonged use of masks). No - mandatory - masks. Also Vaccine/Transmission - relates to lockdowns & masks not working / pointlessness. Also articles re: origin, as this is relevant to incubation time. And maybe some health articles from time to time. Tangential news too.', 'touhou': 'Girls are now preparing, please wait warmly and have some tea. Touhou is a series of danmaku shooters which has am[***]ed a surprisingly active and committed fan following. This subreddit is devoted to sharing the wonderful Touhou series with the Reddit community.', 'LivestreamFail': 'Welcome to /r/LivestreamFail: the place for almost anything livestream related.', 'wicked_edge': 'Wetshaving - isn\\'t all shaving wet? Kinda. Wetshaving is how barbers used to get the ultrasmooth shave of legends. The secret isn\\'t more blades and more canned goo but a single wicked edge. Shaving is 50+ days of a woman\\'s life and 85+ for a man\\'s. Do you want to spend that time hating or enjoying what you are doing? The better way is wetshaving, come and see what \"they\" didn\\'t want you to know.', '[***]og': 'Film photography subreddit. Ask anything about [***]og photography, share photos, discuss techniques, gear or famous photographers.', 'playboicarti': 'A subreddit dedicated to the discussion of hip-hop/trap artist Playboi Carti', 'HolUp': 'Holup! A feeling of shock and confusion. A reaction to an unexpected and often negative turn of events that starkly contrasts the situation\\'s expected outcome. A storytelling element often employed in humor similar to \"bait-and-switch\". jk its all [***]posts. send help pls', 'WTF': 'Things that make you say \"What the ...\".', 'TheRealJoke': 'This subreddit is dedicated to those of us who find the better joke in the comments section of any subreddit or other medium.', 'Anki': \"The unofficial subreddit for the flashcard app Anki. You're welcome to talk about all of the apps and services in the Anki ecosystem here, share resources related to Anki or [***]ed repetition in general, and help each other out with any questions you might have!\", 'xqcow': 'A Reddit community for all things xQc. https://www.twitch.tv/xqcow', 'rutgers': 'The official subreddit for Rutgers University RU RAH RAH', 'worldbuilding': 'For artists, writers, gamemasters, musicians, programmers, philosophers and scientists alike! The creation of new worlds and new universes has long been a key element of speculative fiction, from the fantasy works of Tolkien and Howard, to the science-fiction universes of Burroughs and Asimov, to the tabletop realm of Gygax and Barker, and beyond. This subreddit is about sharing your worlds, discovering the creations of others, and discussing the many aspects of creating new universes.', 'NintendoSwitch': 'The central hub for all news, updates, rumors, and topics relating to the Nintendo Switch. We are a fan-run community, not an official Nintendo forum.', 'BanGDream': 'A subreddit for the multimedia series, BanG Dream!', 'CrackheadCraigslist': 'Half eaten bag of doritos for $2', 'bleach': 'A reddit for the Bleach manga & anime by Tite Kubo.', 'EngineeringStudents': 'This is a place for engineering students of any discipline to discuss study methods, get homework help, get job search advice, and find a comp[***]ionate ear when you get a 40% on your midterm after studying all night.', 'army': 'United States Army on Reddit', 'MensLib': \"The men's issues discussion has been sorely held back by counterproductive tribalism. We're building a new dialogue on the real issues facing men through positivity, inclusiveness, and solutions-building.\", 'intermittentfasting': 'Intermittent Fasting(IF) is way of eating that restricts *when* you eat, usually on a daily or weekly schedule. People engage in IF to reap the many benefits to health, fitness, and mental clarity. This is a place to share success, support each other, ask questions, and learn. IF is an 18+ community because the practice is not medically recommended to/for children.', 'ApexOutlands': 'Meme community for the Free-to-Play Battle Royale game Apex Legends from Res[***] Entertainment', 'SandersForPresident': 'Bernie Sanders for President, dammit!', 'razer': 'Made by redditors, for redditors, to discuss RΛZΞR products. Razer is the world leader in high-performance gaming hardware, software, and systems. Razer support only in the sticky.', 'BrandNewSentence': 'For sentences never before written, found in the wild.', 'Gameboy': 'A subreddit dedicated to discussion of the Nintendo Game Boy. Discussion of all Game Boy models, handheld swaps, mods, games, and homebrew are encouraged.', 'opensource': 'A subreddit for everything [open source](http://en.wikipedia.org/wiki/Open_source) related.', 'brisbane': 'All things Brisbane, Australia!', 'Konosuba': 'The subreddit for Kono Subarashii Sekai ni Shuku[***]u wo!, also known as Konosuba! Also the anime where everyone is useless goddess', 'virginvschad': 'The Virgin walk vs. the Chad stride and expanded universe subreddit', '40kLore': 'A subreddit for the lore and stories encomp[***]ing the dark future of the 41st millennium. Official lore and fan fluff are welcomed. For the best viewing experience, as well as events we recommend using old reddit version - https://old.reddit.com/r/40kLore/', 'PaymoneyWubby': 'The Official PaymoneyWubby subreddit!', 'AMD_Stock': 'Investor strategies and discussion relating to AMD', 'WitchesVsPatriarchy': 'r/WitchesVsPatriarchy is a woman-centered sub with a witchy twist, aimed at healing, supporting, and uplifting one another through humor and magic. The goal is to at once embrace, and poke fun at, the mystical aspects of femininity that have been previously demonized and/or devalued by the Patriarchy.', 'gundeals': '/r/GunDeals is a community dedicated to the collection and sharing of firearm related deals.', 'WindowsMR': 'Place to discuss Windows Mixed Reality platform', 'FigureSkating': 'A community for lovers of figure skating, 花样滑冰, фигурного катания, フィギュアスケート, and\\\\or patinage artistique. Skaters, fans, parents, coaches, and zambonis welcome! See our Wiki for FAQs!', 'l4d2': 'Left 4 Dead series related. Please refer to the sidebar for communities/rules.', 'sportsbook': 'sports betting \"discussion\"', 'Fusion360': 'This sub is for any and all content related to Autodesk AutoCAD Fusion 360. If you need help, if you find a new technique that you love, if you think you have a bug, let us know! We also love it when you post your tips, designs, videos, and general experience with the product and workflow.', 'Norse': '/r/Norse is a subreddit for discussion of Norse and Viking history, mythology, art and culture.', 'Pizza': 'The home of pizza on reddit. An educational community devoted to the art of pizza making.', 'TheVampireDiaries': 'A subreddit for CW television show The Vampire Diaries (2009-2017)', 'criminalminds': \"An elite group of profilers [***]yse the nation's most dangerous criminal minds in an effort to anti[***]te their next moves before they strike again.\", 'sony': 'This subreddit is dedicated to anything and everything Sony-related. There are product(-category) specific subreddits (e.g. r/PlayStation or r/SonyHeadphones), so please consider posting there instead/as'})\n"
          ]
        }
      ],
      "source": [
        "from re import search\n",
        "substring = (\"Covid|Corona\") # create a substring\n",
        "covid_dict = defaultdict(str) # create a dictionary to store the posts\n",
        "for i in range(len(df)): # for each row\n",
        "    if(search(substring, df['subreddit'][i]), re.IGNORECASE): # if the subreddit contains the substring\n",
        "        covid_dict[df['subreddit'][i]] = df['subr_description'][i] # add the subreddit and description to the dictionary\n",
        "    elif(search(substring, df['subr_description'][i]), re.IGNORECASE): # if the description contains the substring\n",
        "        covid_dict[df['subreddit'][i]] = df['subr_description'][i] # add the subreddit and description to the dictionary\n",
        "\n",
        "    else: # if the subreddit does not contain the substring\n",
        "        pass # do nothing    \n",
        "\n",
        "print(covid_dict) # print the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPttp2-fsXG"
      },
      "source": [
        "### P1.2.4 - Redditors that favorite the most\n",
        "\n",
        "Find the users that have favorited the largest number of subreddits. You must produce a pandas dataframe with **two** columns, with the following format:\n",
        "\n",
        "```python\n",
        "     redditor\t    numb_favs\n",
        "0\tuser1           7\n",
        "1\tuser2           6\n",
        "2\tuser3           5\n",
        "3\tuser4           4\n",
        "...\n",
        "```\n",
        "\n",
        "where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LbFeie3jip44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              subreddit  subr_faved_by\n",
            "10          Coronavirus           3324\n",
            "52         playboicarti           2925\n",
            "45           conspiracy           2235\n",
            "41              [***]og           1268\n",
            "25       LivestreamFail           1087\n",
            "..                  ...            ...\n",
            "36          TheRealJoke              6\n",
            "29   NoLockdownsNoMasks              6\n",
            "46        criminalminds              3\n",
            "18  EngineeringStudents              2\n",
            "39            WindowsMR              1\n",
            "\n",
            "[63 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame of subreddit and subr_faved_by from the original dataframe\n",
        "df_subr_faved_by = df[['subreddit','subr_faved_by']]\n",
        "# Create empty lists to store the subreddit name and count subr_faved_by\n",
        "subreddit_list = []\n",
        "subr_faved_by_list = []\n",
        "# Create a dictionary using the list and group by subreddit\n",
        "subr_faved_by_dict = df_subr_faved_by.groupby('subreddit').subr_faved_by.apply(list).to_dict()\n",
        "# Iterate through the dictionary\n",
        "for key, value in subr_faved_by_dict.items():\n",
        "    # Append the subreddit name to the list\n",
        "    subreddit_list.append(key)\n",
        "    # Append the count of subr_faved_by to the list\n",
        "    subr_faved_by_list.append(len(value))\n",
        "# Create a DataFrame using the list\n",
        "df_subr_faved_by = pd.DataFrame({'subreddit':subreddit_list,'subr_faved_by':subr_faved_by_list})\n",
        "# Sort the DataFrame by subr_faved_by\n",
        "df_subr_faved_by = df_subr_faved_by.sort_values(by='subr_faved_by',ascending=False)\n",
        "# Print the DataFrame\n",
        "print(df_subr_faved_by)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "## P1.3 Ethics (8 marks)\n",
        "\n",
        "**(updated on 16/03/2022)**\n",
        "\n",
        "Imagine you are **the head of a data mining company that needs to use** the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). **Some information about the project and the team:**\n",
        "\n",
        " - Your client is a political party concerned about misinformation.\n",
        " - The project requires mining Facebook, Reddit and Instagram data.\n",
        " - The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework. \n",
        "\n",
        "Your answer should address the following:\n",
        "\n",
        " - Identify the action **in which your project is the weakest**.\n",
        " - Then, justify your choice by critically analyzing the three key principles **for that action** outlined in the Framework, namely transparency, accountability and fairness.\n",
        " - Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "---\n",
        "\n",
        "The mainstream and alternative content classifications' URLs were interpreted as stories that supported conspiracy theories. Further investigation revealed that these platforms were either removed or labelled as a conspiracy. Alternative news sources produced more stories that helped conspiracy theories than mainstream news sources. Similar articles from mainstream sources reached a much larger audience. The virality of tales promoting conspiracy ideas was higher than stories denying them. The spread of conspiracy ideas was significantly slowed by content moderation on Facebook, Reddit, Quora and Twitter.\n",
        "\n",
        "1. The evaluation and analysis of broader policy consequences, in my opinion, was indeed weak, with the following explanation: - Conspiracy theories appear on primarily four platforms: Facebook, Twitter, Reddit, and Quora \"politically incorrect\" or \"/pol/\" subsection, which is a popular site for conspiracy theorists. Unlike other occasions, Quora and Reddit are not the only place where conspiracy ideas could be found. It was discovered that stories promoting conspiracy theories went viral faster than debunking or neutralising them. The majority of reports bolstering conspiracy theories came from alternative sources, personal blogs, and social media posts, resulting in many Facebook and twitter likes.\n",
        "\n",
        "2.Based on three fundamentals:\n",
        "\n",
        "a. Transparency, the data published, social media postings made, and available information all have a valid and confirmed source. Twitter and YouTube removed conspiracy-theory-supporting stories, while Reddit and Facebook either removed or flagged them because they were primarily unverified. On Reddit, removing or flagging content was determined by the rules of each sub-community; however, on Facebook, it was decided by whether the company reviewed the stories itself (deleted) or relied on third-party fact-checkers (flagged).\n",
        "\n",
        "b. Accountability—This refers to the presence of effective governance and oversight procedures and control over decisions and actions. It was discovered that content moderation presented varied challenges for each platform. Content moderation on Twitter, for example, was less effective than on other sites. This effect is most likely explained by how content is removed, as disinformation spreads quickly on Twitter in the first few hours after it appears. YouTube had trouble with timing as well. For example, a video claiming that the epidemic is a staged hoax received millions of views in just a few days, with versions of the movie being constantly re-uploaded when it was taken down. Facebook censored the fewest stories that supported conspiracy theories, while Reddit appeared to have no moderation in older content.\n",
        "\n",
        "c. Fairness—Because it is critical to avoid unintended discriminatory effects on individuals or social groups, all biases that may impact the final outcomes should be addressed. These outcomes should respect the dignity of individuals, be non-discriminatory, and be in the public good. Platform owners should pay more attention to what they censor and why they filter it and explain their decisions to users explicitly. More openness and thoughtfulness in material removal, according to studies, makes consumers more aware of the type of information they are consuming, changes how they engage with it and builds trust between them and the services. Mainstream sources should be mindful that the information they generate during the reporting process could be used to support and reinforce the cause of conspiracy theory.\n",
        "\n",
        "\n",
        "Reference: The spread of COVID-19 conspiracy theories on social media and the .... https://misinforeview.hks.harvard.edu/article/the-spread-of-covid-19-conspiracy-theories-on-social-media-and-the-effect-of-content-moderation/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "P1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
