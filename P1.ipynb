{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Data Science Portfolio - Part I (30 marks)\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Pm74v1u4d6G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/joydipb/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/joydipb/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/joydipb/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\") # to ignore warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WfNsDQ253nzJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\n"
          ]
        }
      ],
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "\n",
        "\n",
        "df = pd.read_csv('data_portfolio_21.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CNfbxg2X3nzK"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       author            posted_at  num_comments  score selftext  \\\n",
              "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
              "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
              "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
              "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
              "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
              "\n",
              "  subr_created_at              subr_description  \\\n",
              "0      2009-04-29  Subreddit about Donald Trump   \n",
              "1      2009-04-29  Subreddit about Donald Trump   \n",
              "2      2009-04-29  Subreddit about Donald Trump   \n",
              "3      2009-04-29  Subreddit about Donald Trump   \n",
              "4      2009-04-29  Subreddit about Donald Trump   \n",
              "\n",
              "                                       subr_faved_by  subr_numb_members  \\\n",
              "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "\n",
              "   subr_numb_posts    subreddit  \\\n",
              "0           796986  donaldtrump   \n",
              "1           796986  donaldtrump   \n",
              "2           796986  donaldtrump   \n",
              "3           796986  donaldtrump   \n",
              "4           796986  donaldtrump   \n",
              "\n",
              "                                               title  total_awards_received  \\\n",
              "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
              "1                                Joe Biden's America                      0   \n",
              "2  4 more years and we can erase his legacy for g...                      0   \n",
              "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
              "4                                     LOOK HERE, FAT                      0   \n",
              "\n",
              "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
              "0          1.00            4661         2012-11-09          -0.658599  \n",
              "1          0.67            4661         2012-11-09          -0.658599  \n",
              "2          1.00            4661         2012-11-09          -0.658599  \n",
              "3          1.00            4661         2012-11-09          -0.658599  \n",
              "4          0.88            4661         2012-11-09          -0.658599  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head() # check the first few rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing (10 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUtCUL853Ln"
      },
      "source": [
        "### P1.1.1 - Faved by as lists (3 marks)\n",
        "\n",
        "The column `subr_faved_by` contains an array of values (names of redditors who added the subreddit to which the current post was submitted), but unfortunately they are in text format, and you would not be able to process them properly without converting them to a suitable python type. You must convert these string values to Python lists, going from\n",
        "\n",
        "```python\n",
        "'[\"user1\", \"user2\" ... ]'\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```python\n",
        "[\"user1\", \"user2\" ... ]\n",
        "```\n",
        "\n",
        "**What to implement:** Implement a function `transform_faves(df)` which takes as input the original dataframe and returns the same dataframe, but with one additional column called `subr_faved_by_as_list`, where you have the same information as in `subr_faved_by`, but as a python list instead of a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RJLEddGE56qw"
      },
      "outputs": [],
      "source": [
        "def transform_faves(df):\n",
        "    df['subr_faved_by_as_list'] = [literal_eval(x) for x in df['subr_faved_by']] # this is a list of lists\n",
        "   \n",
        "    return df   # return the dataframe\n",
        "\n",
        "df = transform_faves(df) # run the function on the dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Merge titles and text bodies (4 marks)\n",
        "\n",
        "All Reddit posts need to have a title, but a text body is optional. However, we want to be able to access all free text information for each post without having to look at two columns every time.\n",
        "\n",
        "**What to implement**: A function `concat(df)` that will take as input the original dataframe and will return it with an additional column called `full_text`, which will concatenate `title` and `selftext` columns, but with the following restrictions:\n",
        "\n",
        "- 1) Wrap the title between `<title>` and `</title>` tags.\n",
        "- 2) Add a new line (`\\n`) between title and selftext, but only in cases where you have both values (see instruction 4).\n",
        "- 3) Wrap the selftext between `<selftext>` and `</selftext>`.\n",
        "- 4) You **must not** include the tags in points (1) or (3) if the values for these columns is missing. We will consider a missing value either an empty value (empty string) or a string of only one character (e.g., an emoji). Also, the value of a `full_text` column must not end in the new line character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RsmY-JB39N2m"
      },
      "outputs": [],
      "source": [
        "def concat(df):\n",
        "    df[\"full_text\"] = pd.Series().astype(str) # create a new column\n",
        "    for i in range(len(df)):\n",
        "        if(len(df['selftext'][i]) == 0): # if there is no selftext\n",
        "            df['full_text'][i] = \"<title>\" + df['title'][i] + \"</title>\"   # add the title\n",
        "        else: # if there is selftext\n",
        "            df['full_text'][i] = \"<title>\" + df['title'][i] + \"</title>\\n<selftext>\" + df['selftext'][i] + \"</selftext>\" # add the title and selftext\n",
        "                \n",
        "    return df # return the dataframe\n",
        "    \n",
        "df = concat(df) # run the function on the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADWvbAIe4TVd"
      },
      "source": [
        "### P1.1.3 - Enrich posts (3 marks)\n",
        "\n",
        "We would like to augment our text data with linguistic information. To this end, we will _tokenize_, apply _part-of-speech tagging_, and then we will _lower case_ all the posts.\n",
        "\n",
        "**What to implement**: A function `enrich_posts(df)` that will take as input the original dataframe and will return it with **two** additional columns: `enriched_title` and `enriched_selftext`. These columns will contain tokenized, pos-tagged and lower cased versions of the original text. **You must implement them in this order**, because the pos tagger uses casing information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_nDnaSwI46T_"
      },
      "outputs": [],
      "source": [
        "def enrich_posts(df):\n",
        "    df[\"enriched_title\"] = pd.Series().astype(str) # create a new column\n",
        "    df[\"enriched_selftext\"] = pd.Series().astype(str) # create a new column\n",
        "    for i in range(len(df)):\n",
        "        df['enriched_title'][i] = word_tokenize(text=df['title'][i]) # tokenize the title\n",
        "        df['enriched_title'][i] = nltk.pos_tag(df['enriched_title'][i]) # tag the title\n",
        "        df['enriched_title'][i] = [(ls.lower(), cat) for ls, cat in df['enriched_title'][i]] # lowercase the title and tag it\n",
        "\n",
        "        df['enriched_selftext'][i] = word_tokenize(text=df['selftext'][i]) # tokenize the selftext\n",
        "        df['enriched_selftext'][i] = nltk.pos_tag(df['enriched_selftext'][i]) # tag the selftext\n",
        "        df['enriched_selftext'][i] = [(ls.lower(), cat) for ls, cat in df['enriched_selftext'][i]] # lowercase the selftext and tag it \n",
        "    return df # return the dataframe\n",
        "\n",
        "df = enrich_posts(df) # run the function on the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (12 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Users with best scores (3 marks)\n",
        "\n",
        "- Find the users with the highest aggregate scores (over all their posts) for the whole dataset. You should restrict your results to only those whose aggregated score is above 10,000 points, in descending order. Your code should generate a dictionary of the form `{author:aggregated_scores ... }`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhW8Rr6QSXDj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'DaFunkJunkie': 250375, 'Dajakesta0624': 11613, 'JLBesq1981': 58235, 'None': 218846, 'NotsoPG': 18518, 'OldFashionedJizz': 64398, 'SUPERGUESSOUS': 211611, 'SonictheManhog': 18116, 'TheGamerDanYT': 25357, 'TheJeck': 26058, 'TrumpSharted': 21154, 'Wagamaga': 47989, 'chrisdh79': 143538, 'hildebrand_rarity': 122464, 'hilltopye': 81245, 'iSlingShlong': 118595, 'jigsawmap': 210784, 'rspix000': 57107, 'stem12345679': 47455, 'tefunka': 79560, 'BlanketMage': 13677, 'NewAltWhoThis': 12771, 'apocalypticalley': 10382, 'kevinmrr': 11900})\n"
          ]
        }
      ],
      "source": [
        "df_preserve = df.copy() # make a copy of the dataframe\n",
        "import gc\n",
        "for i in range(len(df)-1): # for each row\n",
        "    if(df['author'][i+1] == df['author'][i]): # if the next row is the same author\n",
        "        df['score'][i+1] += df['score'][i] # add the score\n",
        "        df = df.drop(index=i) # drop the current row\n",
        "        \n",
        "    else: # if the next row is not the same author\n",
        "        pass # do nothing\n",
        "\n",
        "df = df.reset_index() # reset the index\n",
        "\n",
        "best_score_dict = defaultdict(int) # create a dictionary\n",
        "\n",
        "for i in range(len(df)): # for each row\n",
        "    if(df['score'][i] >= 10000): # if the score is greater than 10000\n",
        "        best_score_dict[df['author'][i]] += df['score'][i] # add the score to the dictionary\n",
        "                \n",
        "    else: # if the score is less than 10000\n",
        "        pass # do nothing\n",
        "\n",
        "df = df_preserve.copy() # make a copy of the dataframe\n",
        "\n",
        "# Sort the dictionary by value\n",
        "best_score_dict = sorted(best_score_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "del df_preserve # delete the copy\n",
        "gc.collect() # garbage collect\n",
        "\n",
        "print(best_score_dict) # print the dictionary\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOFrPFQT5cZ"
      },
      "source": [
        "### P1.2.2 - Awarded posts (3 marks)\n",
        "\n",
        "Find the number of posts that have received at least one award. Your query should return only one value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fVuaWmmUGVW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of post that recieved atleast one award are:  119\n"
          ]
        }
      ],
      "source": [
        "award_count = 0 # create a counter\n",
        "for i in range(len(df)): # for each row\n",
        "    if(df['total_awards_received'][i] >= 1): # if the award count is greater than 1\n",
        "        award_count+=1 # add 1 to the counter\n",
        "    else: # if the award count is less than 1\n",
        "        pass # do nothing\n",
        "print(\"Number of post that recieved atleast one award are: \", award_count) # print the counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 Find Covid (3 marks)\n",
        "\n",
        "Find the name and description of all subreddits where the name starts with `Covid` or `Corona` and the description contains `covid` or `Covid` anywhere. Your code should generate a dictionary of the form#\n",
        "\n",
        "```python\n",
        "  {'Coronavirus':'Place to discuss all things COVID-related',\n",
        "  ...\n",
        "  }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w6fIWO8BUhu3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/joydipb/Library/CloudStorage/OneDrive-CardiffUniversity/CMT309_Comp_DS/P1.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joydipb/Library/CloudStorage/OneDrive-CardiffUniversity/CMT309_Comp_DS/P1.ipynb#ch0000018?line=1'>2</a>\u001b[0m substring \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mCovid|Corona\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# create a substring\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joydipb/Library/CloudStorage/OneDrive-CardiffUniversity/CMT309_Comp_DS/P1.ipynb#ch0000018?line=2'>3</a>\u001b[0m covid_dict \u001b[39m=\u001b[39m defaultdict(\u001b[39mstr\u001b[39m) \u001b[39m# create a dictionary to store the posts\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joydipb/Library/CloudStorage/OneDrive-CardiffUniversity/CMT309_Comp_DS/P1.ipynb#ch0000018?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)): \u001b[39m# for each row\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joydipb/Library/CloudStorage/OneDrive-CardiffUniversity/CMT309_Comp_DS/P1.ipynb#ch0000018?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m(search(substring, df[\u001b[39m'\u001b[39m\u001b[39msubreddit\u001b[39m\u001b[39m'\u001b[39m][i]), re\u001b[39m.\u001b[39mIGNORECASE): \u001b[39m# if the subreddit contains the substring\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joydipb/Library/CloudStorage/OneDrive-CardiffUniversity/CMT309_Comp_DS/P1.ipynb#ch0000018?line=5'>6</a>\u001b[0m         covid_dict[df[\u001b[39m'\u001b[39m\u001b[39msubreddit\u001b[39m\u001b[39m'\u001b[39m][i]] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msubr_description\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m# add the subreddit and description to the dictionary\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "from re import search\n",
        "substring = (\"Covid|Corona\") # create a substring\n",
        "covid_dict = defaultdict(str) # create a dictionary to store the posts\n",
        "for i in range(len(df)): # for each row\n",
        "    if(search(substring, df['subreddit'][i]), re.IGNORECASE): # if the subreddit contains the substring\n",
        "        covid_dict[df['subreddit'][i]] = df['subr_description'][i] # add the subreddit and description to the dictionary\n",
        "    elif(search(substring, df['subr_description'][i]), re.IGNORECASE): # if the description contains the substring\n",
        "        covid_dict[df['subreddit'][i]] = df['subr_description'][i] # add the subreddit and description to the dictionary\n",
        "\n",
        "    else: # if the subreddit does not contain the substring\n",
        "        pass # do nothing    \n",
        "\n",
        "print(covid_dict) # print the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPttp2-fsXG"
      },
      "source": [
        "### P1.2.4 - Redditors that favorite the most\n",
        "\n",
        "Find the users that have favorited the largest number of subreddits. You must produce a pandas dataframe with **two** columns, with the following format:\n",
        "\n",
        "```python\n",
        "     redditor\t    numb_favs\n",
        "0\tuser1           7\n",
        "1\tuser2           6\n",
        "2\tuser3           5\n",
        "3\tuser4           4\n",
        "...\n",
        "```\n",
        "\n",
        "where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbFeie3jip44"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame of subreddit and subr_faved_by from the original dataframe\n",
        "df_subr_faved_by = df[['subreddit','subr_faved_by']]\n",
        "# Create empty lists to store the subreddit name and count subr_faved_by\n",
        "subreddit_list = []\n",
        "subr_faved_by_list = []\n",
        "# Create a dictionary using the list and group by subreddit\n",
        "subr_faved_by_dict = df_subr_faved_by.groupby('subreddit').subr_faved_by.apply(list).to_dict()\n",
        "# Iterate through the dictionary\n",
        "for key, value in subr_faved_by_dict.items():\n",
        "    # Append the subreddit name to the list\n",
        "    subreddit_list.append(key)\n",
        "    # Append the count of subr_faved_by to the list\n",
        "    subr_faved_by_list.append(len(value))\n",
        "# Create a DataFrame using the list\n",
        "df_subr_faved_by = pd.DataFrame({'subreddit':subreddit_list,'subr_faved_by':subr_faved_by_list})\n",
        "# Sort the DataFrame by subr_faved_by\n",
        "df_subr_faved_by = df_subr_faved_by.sort_values(by='subr_faved_by',ascending=False)\n",
        "# Print the DataFrame\n",
        "print(df_subr_faved_by)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "## P1.3 Ethics (8 marks)\n",
        "\n",
        "**(updated on 16/03/2022)**\n",
        "\n",
        "Imagine you are **the head of a data mining company that needs to use** the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). **Some information about the project and the team:**\n",
        "\n",
        " - Your client is a political party concerned about misinformation.\n",
        " - The project requires mining Facebook, Reddit and Instagram data.\n",
        " - The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework. \n",
        "\n",
        "Your answer should address the following:\n",
        "\n",
        " - Identify the action **in which your project is the weakest**.\n",
        " - Then, justify your choice by critically analyzing the three key principles **for that action** outlined in the Framework, namely transparency, accountability and fairness.\n",
        " - Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "---\n",
        "\n",
        "The mainstream and alternative content classifications' URLs were interpreted as stories that supported conspiracy theories. Further investigation revealed that these platforms were either removed or labelled as a conspiracy. Alternative news sources produced more stories that helped conspiracy theories than mainstream news sources. Similar articles from mainstream sources reached a much larger audience. The virality of tales promoting conspiracy ideas was higher than stories denying them. The spread of conspiracy ideas was significantly slowed by content moderation on Facebook, Reddit, Quora and Twitter.\n",
        "\n",
        "1. The evaluation and analysis of broader policy consequences, in my opinion, was indeed weak, with the following explanation: - Conspiracy theories appear on primarily four platforms: Facebook, Twitter, Reddit, and Quora \"politically incorrect\" or \"/pol/\" subsection, which is a popular site for conspiracy theorists. Unlike other occasions, Quora and Reddit are not the only place where conspiracy ideas could be found. It was discovered that stories promoting conspiracy theories went viral faster than debunking or neutralising them. The majority of reports bolstering conspiracy theories came from alternative sources, personal blogs, and social media posts, resulting in many Facebook and twitter likes.\n",
        "\n",
        "2.Based on three fundamentals:\n",
        "\n",
        "a. Transparency, the data published, social media postings made, and available information all have a valid and confirmed source. Twitter and YouTube removed conspiracy-theory-supporting stories, while Reddit and Facebook either removed or flagged them because they were primarily unverified. On Reddit, removing or flagging content was determined by the rules of each sub-community; however, on Facebook, it was decided by whether the company reviewed the stories itself (deleted) or relied on third-party fact-checkers (flagged).\n",
        "\n",
        "b. Accountability—This refers to the presence of effective governance and oversight procedures and control over decisions and actions. It was discovered that content moderation presented varied challenges for each platform. Content moderation on Twitter, for example, was less effective than on other sites. This effect is most likely explained by how content is removed, as disinformation spreads quickly on Twitter in the first few hours after it appears. YouTube had trouble with timing as well. For example, a video claiming that the epidemic is a staged hoax received millions of views in just a few days, with versions of the movie being constantly re-uploaded when it was taken down. Facebook censored the fewest stories that supported conspiracy theories, while Reddit appeared to have no moderation in older content.\n",
        "\n",
        "c. Fairness—Because it is critical to avoid unintended discriminatory effects on individuals or social groups, all biases that may impact the final outcomes should be addressed. These outcomes should respect the dignity of individuals, be non-discriminatory, and be in the public good. Platform owners should pay more attention to what they censor and why they filter it and explain their decisions to users explicitly. More openness and thoughtfulness in material removal, according to studies, makes consumers more aware of the type of information they are consuming, changes how they engage with it and builds trust between them and the services. Mainstream sources should be mindful that the information they generate during the reporting process could be used to support and reinforce the cause of conspiracy theory.\n",
        "\n",
        "\n",
        "Reference: The spread of COVID-19 conspiracy theories on social media and the .... https://misinforeview.hks.harvard.edu/article/the-spread-of-covid-19-conspiracy-theories-on-social-media-and-the-effect-of-content-moderation/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "P1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
