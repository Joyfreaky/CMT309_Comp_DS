{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8wldkmAnEVA"
   },
   "source": [
    "# Data Science Portfolio - Part II (40 marks)\n",
    "\n",
    "This question has been created to test your statistical analysis and programming knowledge in Python. \n",
    "\n",
    "You are given a `csv` file, which include various data entries for each football match in **English Premier League** during the 2020-2021 season. To name a few of these entries: date, referee name, number of goals, red cards, etc. The `csv` dataset you are provided contains one row per football match. The column names are abbreviations and given as: \n",
    "\n",
    "```\n",
    "Div = League Division\n",
    "Date = Match Date (dd/mm/yy)\n",
    "Time = Time of match kick off\n",
    "HomeTeam = Home Team\n",
    "AwayTeam = Away Team\n",
    "FTHG = Full Time Home Team Goals\n",
    "FTAG = Full Time Away Team Goals\n",
    "FTR = Full Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "HTHG = Half Time Home Team Goals\n",
    "HTAG = Half Time Away Team Goals\n",
    "HTR = Half Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "Referee = Match Referee\n",
    "HS = Home Team Shots\n",
    "AS = Away Team Shots\n",
    "HST = Home Team Shots on Target\n",
    "AST = Away Team Shots on Target\n",
    "HF = Home Team Fouls Committed\n",
    "AF = Away Team Fouls Committed\n",
    "HC = Home Team Corners\n",
    "AC = Away Team Corners\n",
    "HY = Home Team Yellow Cards\n",
    "AY = Away Team Yellow Cards\n",
    "HR = Home Team Red Cards\n",
    "AR = Away Team Red Cards\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsEbFWRWxruW"
   },
   "source": [
    "In this exercise, you are asked to perform a number of operations to:\n",
    "\n",
    " - perform statistical analysis of the data, and\n",
    "\n",
    " - gain insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMkSf1mxmwOX"
   },
   "outputs": [],
   "source": [
    "# suggested imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from urllib import request\n",
    "import scipy.stats as stats\n",
    "from statsmodels import graphics\n",
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "from pymc3 import glm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, RocCurveDisplay, auc, roc_curve\n",
    "import seaborn as sns\n",
    "sns.set_style(style=\"darkgrid\", rc={\"axes.facecolor\": \".9\", \"grid.color\": \".8\"})\n",
    "sns.set_palette(palette=\"deep\")\n",
    "sns_c = sns.color_palette(palette=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzaUcdkbnr31"
   },
   "outputs": [],
   "source": [
    "module_url = f\"https://raw.githubusercontent.com/oktaykarakus/cmt309-portfolio/main/EPL_season-2021.csv\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))\n",
    "\n",
    "df = pd.read_csv('EPL_season-2021.csv')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Felq9IkwMkZT"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj9f0ZhuobQZ"
   },
   "source": [
    "## P2.1 - Data Pre-processing and Exploratory Analysis (11 marks)\n",
    "\n",
    "In this question, your task is to use `pandas` and other required modules to preprocess the data frame, `df`. Preprocessing will include: add/remove/recode columns in `df`. In addition, to further explore the dataset, you need to produce a number of exploratory plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ixhqXsfobZ_"
   },
   "source": [
    "#### P2.1.1 - Add Booking Points Columns (1 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0Jf29QCp4zs"
   },
   "source": [
    "Sometimes, in order to better analyse any given data set, one can create a new type of feature by combining two or more existing entries of the data frame. In this question, you are asked to create a function `add_booking_pts(df)` which creates two new columns of: **Home booking points (HBP)**, and **Away booking points (ABP)** by using four existing columns of HY, AY, HR, and AR.\n",
    "\n",
    "The details of the function `add_booking_pts(df)` are given below:\n",
    "\n",
    " - Takes the data frame `df` as input.\n",
    " \n",
    " - For each match, number of yellow cards is weighted with 10 points, whilst the number of red cards is with 25 points. \n",
    " \n",
    " - Basically, the function calculates HBP and ABP columns as\n",
    "    - $HBP = 10\\cdot HY + 25\\cdot HR$\n",
    "    - $ABP = 10\\cdot AY + 25\\cdot AR$\n",
    " \n",
    " - These newly created arrays are added to `df`, whilst removing the columns for HY, AY, HR, and AR.\n",
    " \n",
    " - Finally, the updated `df` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXl1C82Yp4zt"
   },
   "outputs": [],
   "source": [
    "def add_booking_pts(df):\n",
    "    # your code here\n",
    "    return df\n",
    "add_booking_pts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpEDOaYYp4zt"
   },
   "source": [
    "#### P2.1.2 - Convert Table Colums into Digits (2 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-1rXZEEp4zu"
   },
   "source": [
    "When reading in the dataframe, one can see that it contains some textual data which will not be relevant for the numerical analyses in Question 1. Therefore, implement a function `convert_results(df)` \n",
    "\n",
    "1. (1 mark) to convert **half-time results (HTR)** and **full-time results (FTR)** into numerical data. The details of the function are given below: \n",
    " \n",
    "- HTR and FTR columns include string values of `'H'`, `'D'` and `'A'`. These string corresponds to the cases below:\n",
    "    - `'H'`: Home team win\n",
    "\n",
    "    - `'D'`: Draw\n",
    "\n",
    "    - `'A'`: Away team win\n",
    "\n",
    "- The function `convert_results(df)` will replace `'H'`, `'D'` and `'A'` values with `int` type values of of 1, 0, -1, respectively.\n",
    "\n",
    "2. (1 mark) to convert **Time** column into `float` type values in interval of $[0, 24)$. Since an hour has 60 minutes, a 15-minute interval corresponds to quarter of an hour (i.e 0.25 hours). Considering this, some examples can be given:\n",
    " \n",
    "- `'12.30'` will be `12.5`, or \n",
    " \n",
    "- `'18.15'` will be `18.25` or \n",
    " \n",
    "- `'17.00'` will be `17.0`\n",
    " \n",
    "The function `convert_results(df)` should return the updated data frame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXXPlfyQp4zu"
   },
   "outputs": [],
   "source": [
    "def convert_results(df):\n",
    "    # your code here\n",
    "    return df\n",
    "convert_results(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvLkIWJCpmbb"
   },
   "source": [
    "#### P2.1.3 - Fair Play League Table (4 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKfgRnstp4zy"
   },
   "source": [
    "Write a function `create_fair_play_table(df)` to depict a **barplot with horizontal bars** representing *total booking points*. The module `seaborn` and its corresponding methods and attributes must be used in this question.\n",
    "\n",
    "Produce a bar for each team (use the HBP and ABP columns) and plot them in increasing order. The team with the top bar (*i.e.* lowest HBP+ABP) will represent the Fair Play League Champion. Also, print the statement below\n",
    "\n",
    "```\n",
    "The champions of the 2020-2021 Fair Play League is {locate the team here}.\n",
    "```\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=11gYwhu-1D6dxEcmXK31pL5Zcop497w9i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWfFEYxRp4zy"
   },
   "outputs": [],
   "source": [
    "def create_fair_play_table(df):\n",
    "    # your code here\n",
    "    return df_fp\n",
    "df_fp = create_fair_play_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdfxa8tcp4zy"
   },
   "source": [
    "### P2.1.4 - Expected number of goals vs. Exact scores (4 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWnCZqXBp4zy"
   },
   "source": [
    "Expected number of goals (xG) is a predictive model used to assess every goal-scoring chance, and the likelihood of scoring. Nowadays, xG has a strong algorithm behind it, but in this example, we will assume two basic models to calculate the xG of any football team for any game:\n",
    " \n",
    " - $xG_1 = 0.1 \\cdot S$\n",
    " \n",
    " - $xG_2 = 0.1 \\cdot S + 0.05\\cdot SoT$\n",
    "\n",
    "where $S$ and $SoT$ is the total number of shoots and total shoots on target, respectively.\n",
    "\n",
    "Write a function `xG_plot(df)` to plot $xG_1$, $xG_2$ and exact number of goals scored by a randomly selected football team for whole 38 game weeks. \n",
    "\n",
    "This exploratory analysis will depict two sub-plots:\n",
    " \n",
    " 1. Running mean of all three variables (xG1, xG2 and Goals). Running mean shows the average of a value up to a time step. For example: running mean of xG1 for week 5 is $\\frac{1}{5}\\sum_{i=0}^{4}xG1_{i}$, or similarly for week 17 is $\\frac{1}{17}\\sum_{i=0}^{16}xG1_{i}$.\n",
    "\n",
    " 2. Cumulative sum of all three variables (xG1, xG2 and Goals). (Hint: `np.cumsum()`)\n",
    "\n",
    "Use different colours, line styles, legends, etc. to make the exploratory analysis more understandable. The function also returns calculated xGs for the randomly selected team in a `dict` type object of form `{team : (xG1, xG2)}`\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Y3uSzRQaKumgoKoNdkohYBhpgBrOkYeO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxOzvXRgp4zy"
   },
   "outputs": [],
   "source": [
    "def xG_plot(df):\n",
    "    # your code here\n",
    "    return {team : (xG1, xG2)} # just for an example. your result might differ from this definition.\n",
    "xG = xG_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi2EF20mpms-"
   },
   "source": [
    "## P2.2 - Statistical Analysis (29 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ0WOMWuQwau"
   },
   "source": [
    "#### P2.2.1 - Model selection for Regression Analysis (9 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RUYzPjN9u3j"
   },
   "source": [
    "In this question, we construct a regression analyses to investigate how well FTHG (or FTAG) can be predicted from the other variables in the dataframe. The objective of this question is to derive a sparse model (linear and polynomial) with fewer variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWR2-QUE9zMS"
   },
   "source": [
    "#### P2.2.1.1 - Variable Selection for Linear Regression (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsTqLMFYQ88W"
   },
   "source": [
    "In **variable selection** ('variable' means the same as 'predictor'), variables get iteratively added or removed from the regression model. Once finished, the model typically contains only a subset of the original variables. It makes it easier to interpret the model, and in some cases it makes it generalise better to new data. \n",
    "\n",
    "To perform variable selection, create a function `select_variable(df, main_pred, main_target, alpha)`, where \n",
    " \n",
    " - `main_pred` is a dictionary of variables. For this analysis, firstly, either all Home or Away teams will be marked and the predictors given below will be used\n",
    "\n",
    "  - Home: [Time, FTR, HTHG, HTR, HS, HST, HF, HC, HBP]\n",
    "  \n",
    "  - Away: [Time, FTR, HTAG, HTR, AS, AST, AF, AC, ABP]. \n",
    " \n",
    " - `main_target` is the variable for the regression, Home: FTHG (or Away: FTAG)\n",
    " \n",
    " - `alpha` is the significance level for selecting significant predictors\n",
    "\n",
    "The function should return\n",
    "\n",
    " - `main_pred` is the dictionary which stores the selected subset of initial `main_pred` both for home and away teams, in a format of `main_pred = {'Home': [... selected predictors here ...], 'Away': [... selected predictors here ...]}`.\n",
    "\n",
    "To calculate regression fits and $p$-values you will use `statsmodels`. The general procedure follows two stages:\n",
    "\n",
    " - Stage 1 (adding predictors): you build a model by adding variables one after the other. You keep adding variables that increase the **adjusted $R^2$** value (provided by `statsmodels` package). \n",
    "  \n",
    "  - Start with an empty set of variables\n",
    "  \n",
    "  - Fit multiple one-variable regression models. In each iteration, use one of the variables provided in predictors. The variable that leads to the largest increase in adjusted $R^2$ is added to the model.\n",
    "  \n",
    "  - Now proceed by adding a second variable into the model. Starting from the remaining variables, again choose the variable that leads to the largest increase in adjusted $R^2$.\n",
    "  \n",
    "  - Continue in the same way for the third, fourth, … variable.\n",
    "  \n",
    "  - You are finished when there is no variable left that increases adjusted $R^2$.\n",
    " \n",
    " - Stage 2 (removing non-significant predictors): if any of the utilised predictors are not significant, you need to remove them. Keep removing variables until all variables in the model are significant.\n",
    "\n",
    "  - Start by fitting a model using the variables that have been added to the model in Stage 1.\n",
    "  \n",
    "  - If there is a variable that is not significant, remove the variable with the largest $p$-value and fit the model again with the reduced set of variables.\n",
    "  \n",
    "  - Keep removing variables and re-fitting the model until all remaining variables are significant.\n",
    "  \n",
    "  - The remaining significant variables are the output of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EyQlNHIQ9um"
   },
   "outputs": [],
   "source": [
    "def select_variable(df, main_pred, main_target, alpha):\n",
    "    # your code here\n",
    "    return main_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTghkn-V-KKh"
   },
   "source": [
    "#### P2.2.1.2 - Model Selection for Polynomial Regression (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQzKiMLF-KYZ"
   },
   "source": [
    "Often the dataset provided is not linearly separable and a simple linear regression model may not be able to derive relationships between both the independent and dependent variables. In such cases, a possible solution would be to implement polynomial regression instead (https://en.wikipedia.org/wiki/Polynomial_regression). Polynomial regression is a form of regression analysis in which the relationship between the independent variable $x$ and the dependent variable $y$ is modelled as an $n^{th}$ degree polynomial in $x$.\n",
    "\n",
    "**Example:** Given $y$ the dependent variable, $x_1, x_2$ the independent variables, $b_0$ the bias and $b_1,b_2,...,b_n$ the weights a polynomial regression of degree 2 would have the form:\n",
    "\n",
    "$$y = b_0 + b_1x_1 + b_2x_1^2 + b_3x_2 + b_4x_2^2$$\n",
    "\n",
    "Implement a function `polynomial_model(df, main_pred, main_target, degrees)` which uses the selected subset of variables as an argument from the function `select_variable()`, and calculates all possible combinations of the variable set and polynomial degrees. The function `polynomial_model()` finds the degree that yields the best polynomial model (according to the adjusted R-squared metric) to predict the value of a FTHG or FTAG as in the linear regression part above.\n",
    "\n",
    "Arguments and outputs of the function are given as\n",
    "\n",
    " - a dataframe `df`, \n",
    "\n",
    " - a dictionary `main_pred` indicating the predictors for home and away, \n",
    " \n",
    " - a dictionary `main_target` indicating target variable for home and away, \n",
    " \n",
    " - a list of integers indicating the degrees to test degrees, \n",
    " \n",
    "The function should return \n",
    "\n",
    " - the best fitted regression model, and best polynomial degree for home and away in a dictionary `main_predp` of format `main_predp = {'Home': (best_fit, best_degree), 'Away': (best_fit, best_degree)}`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQr38k8uu2Tj"
   },
   "outputs": [],
   "source": [
    "def polynomial_model(df, main_pred, main_target, degrees):\n",
    "    # your code here\n",
    "    return main_predp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--f081zRQ-Jn"
   },
   "source": [
    "#### P2.2.2 - Predicting Match Result (5 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dgA68M4RLW2"
   },
   "source": [
    "Create a function `predict_result()` which predicts the result of **Man City - Everton** football match which was played on 23/05/2021. In order to do this, firstly crop last last 10 rows of the data frame `df` to use only the first 37 weeks (370 matches) of the season to fit your regressors.\n",
    "\n",
    "The function `predict_result()` will use `select_variable()` and `polynomial_model()` function outputs as the best linear and polynomial regression models. Then by using these two models, it predicts the number of goals scored by Home and Away teams separately, which will lead to the result of the match. Finally, print the information below:\n",
    "\n",
    "```\n",
    "Linear regression prediction        : Man City x - y Everton\n",
    "Polynomial regression prediction    : Man City a - b Everton\n",
    "Correct result                      : Man City 5 - 0 Everton\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVSNW9aNRLGV"
   },
   "outputs": [],
   "source": [
    "def predict_result(df, main_pred, main_predp, team1 = 'Man City', team2 = 'Everton'):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyRqtizxRu3G"
   },
   "source": [
    "#### P2.2.3 - Maximum likelihood estimation (MLE) and prediction (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-DP2id4u6OZ"
   },
   "source": [
    "In this question, you are expected to solve a regression problem, but this time using **maximum likelihood estimation (MLE)** theory. You need to construct a regression analysis to investigate how well the **full time results (FTR)** can be predicted from the other variables of FTHG, HS, HC, AS, AC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe-mKUDpRNyF"
   },
   "source": [
    "#### P2.2.3.1 - ML Estimate of regression parameters (7 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLtz9BKgRODV"
   },
   "source": [
    "Create a function `ML_estimate(df_est, pred, target)` which calculates the ordinary least squares (OLS) and two MLE fits (Poisson and Probit) for the given arguments.\n",
    "\n",
    "You need to use `statsmodels` module and its corresponding methods of `.OLS()`, `.Poisson()` and `.Probit()`.\n",
    "\n",
    "- `df_est` is a subset of the data frame `df` which includes **randomly selected** 280 rows of `df`. The remaining 100 rows will be used in prediction application (see below).\n",
    "\n",
    "- `pred` is a list of variables. For this analysis, OLS and other models utilise the predictors of FTHG, HS, HC, AS, AC. (Note: Depending on your implementation, you might need to add a constant to the predictors. Please see the lecture notes)\n",
    "\n",
    "- `target` is the target variable for the regression, FTR. You need to adjust values of this column for the purpose of this question. \n",
    " \n",
    "  - FTR = 1.0 if Home team wins.\n",
    "  \n",
    "  - FTR = 0.0 if Away team wins or a Draw.\n",
    "\n",
    "The function should return variables \n",
    "\n",
    " - `MLE_model_fits` a `tuple` object which stores all three model fits `statsmodels` objects for OLS, Probit and Poisson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgaDaJv7nEfB"
   },
   "outputs": [],
   "source": [
    "def ML_estimate(df_est, pred, target):\n",
    "    # your code here\n",
    "    return MLE_model_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or-Bub43ujLf"
   },
   "source": [
    "#### P2.2.3.2 - Predicting Home Win via MLE (3 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E25CPvBuk3x"
   },
   "source": [
    "Create a function `ML_predict(df_pred, MLE_model_fits)` which calculates FTR predictions for all three models of OLS, Probit and Poisson.\n",
    "\n",
    "You need to use `statsmodels`' method for prediction: `.predict()`.\n",
    "\n",
    "- `df_pred` is a subset of the data frame `df` which includes only a subset of 100 rows of `df`.\n",
    "\n",
    "- `MLE_model_fits` is a `tuple` object obtained from the `ML_estimate()` function above. Unpack this argument to obtain `statsmodels` objects for all three models. \n",
    "\n",
    "This function should return: \n",
    "\n",
    " - `df_pred`\n",
    "\n",
    " - `MLE_model_predictions` is a `tuple` which stores the predicted outputs for each three models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7v0YyQsulIV"
   },
   "outputs": [],
   "source": [
    "def ML_predict(df_pred, MLE_model_fits):\n",
    "    # your code here\n",
    "    return MLE_model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB27Qtnavr2M"
   },
   "source": [
    "#### P2.2.4 - Evaluating Prediction Performance (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX7gMCQ_vsA5"
   },
   "source": [
    "You will now need to visualise the prediction performance of the models, and evaluate them in terms of prediction accuracy, and mean square error (MSE) metrics. For this purpose, create a function `prediction_perf(gt, MLE_model_predictions)` which evaluates the prediction performance of the reference models. Up to this point, you should have obtained\n",
    "\n",
    " - $N = 100$ samples of predictions from each model, stored in `MLE_model_predictions`.\n",
    " \n",
    " - The ground-truth FTR values from data frames `df`, stored in `gt`.\n",
    "\n",
    "Assume predicted values for a given model are stored in a variable $P$ and its average is $\\bar{P}$. The first performance measure will be the MSE, and will be calculated for each model from the expression below:\n",
    "\n",
    "$$ MSE = \\dfrac{1}{N}\\sum_{i=0}^{N-1} (P_i - FTR_i)^2$$\n",
    "\n",
    "In order to obtain the prediction accuracy for each model, you first should convert continuous prediction results into the binary form (either 1.0 or 0.0). The binarisation process will follow the piecewise function below:\n",
    "\n",
    "  $$P_{binary, i} = \\begin{cases} 1.0, & P_i \\geq \\bar{P}\\\\ 0.0, & \\text{otherwise}  \\end{cases} \\quad \\text{where} \\quad i = 0, 1, \\dots, 99$$\n",
    "\n",
    "Then the percentage accuracy, $Acc\\%$ is calculated as\n",
    "\n",
    "$$ Acc\\% = 100 - \\sum_{i=0}^{99} |P_{binary, i} - FTR_i|$$\n",
    "\n",
    "Following these, by using `sklearn` module methods `roc_curve()` and `auc()` find ROC curve parameters and AUC metric for each prediction model. \n",
    "\n",
    "In order to obtain performance analysis results in a neatly way, you then need to create a new `pandas` dataframe `df_results` which will be in the form of\n",
    "\n",
    "```\n",
    "+----+-------------+--------+--------+-------+\n",
    "|    | Model       |   Acc% |    MSE |   AUC |\n",
    "+====+=============+========+========+=======+\n",
    "|  0 | OLS         |  77.00 | 0.1260 | 0.911 |\n",
    "+----+-------------+--------+--------+-------+\n",
    "|  1 | MLE-Probit  |  81.00 | 0.1086 | 0.911 |\n",
    "+----+-------------+--------+--------+-------+\n",
    "|  2 | MLE-Poisson |  76.00 | 0.1490 | 0.884 |\n",
    "+----+-------------+--------+--------+-------+\n",
    "```\n",
    "\n",
    "Consequently, using `sklearn` method `RocCurveDisplay()`, the `prediction_perf()` function should \n",
    "\n",
    " - `print` and `return` the data frame `df_results`.\n",
    "\n",
    "**Marking for this question**\n",
    " - (2 marks) Calculating MSE and $Acc\\%$ metrics correctly.\n",
    " - (3 marks) Creating and returning dataframe `df_results`.\n",
    " \n",
    "with a condition that all these three operations are performed in a **fully working `prediction_perf()` function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sSLn8E92ue3"
   },
   "outputs": [],
   "source": [
    "def prediction_perf(gt, MLE_model_predictions):\n",
    "    # your code here\n",
    "    return df_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "P2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
